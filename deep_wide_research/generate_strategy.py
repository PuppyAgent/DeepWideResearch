"""Report generation stage strategy for Deep Research.

This module encapsulates the final report creation using an LLM, based on
messages, research brief (optional), and findings collected during research.
"""

from __future__ import annotations

from typing import Dict, List, Optional

# Support direct execution and module imports - try absolute and relative imports
try:
    # Try importing as part of the package (development environment)
    from .newprompt import final_report_generation_prompt
    from .providers import chat_complete
except ImportError:
    # Try absolute imports (direct run or deployment environment)
    try:
        from deep_wide_research.newprompt import final_report_generation_prompt
        from deep_wide_research.providers import chat_complete
    except ImportError:
        # Import as standalone modules (Railway deployment environment)
        from newprompt import final_report_generation_prompt
        from providers import chat_complete


def _today_str() -> str:
    import datetime as _dt

    now = _dt.datetime.now()
    return f"{now:%a} {now:%b} {now.day}, {now:%Y}"


async def generate_report(state: Dict, cfg, api_keys: Optional[dict] = None) -> str:
    """Generate the final report and return its content string.

    Side effect: none on input state; returns the report so caller can
    decide how to persist it.
    """
    findings = "\n".join(state.get("notes") or [])
    system_message = {"role": "system", "content": final_report_generation_prompt.format(date=_today_str())}
    # User message: includes the original user question and raw_notes JSON (injected into messages by engine)
    user_payload = {
        "role": "user",
        "content": (
            "<Messages>\n" +
            "\n\n".join(f"{m['role'].upper()}: {m['content']}" for m in state.get("messages", [])) +
            "\n</Messages>\n\n" +
            "<Findings>\n" + findings + "\n</Findings>"
        )
    }

    # Optional: print debug logs
    print(system_message["content"])
    print(user_payload["content"])

    resp = await chat_complete(
        cfg.final_report_model,
        [system_message, user_payload],
        cfg.final_report_model_max_tokens,
        api_keys,
    )
    return resp.content


async def generate_report_stream(state: Dict, cfg, api_keys: Optional[dict] = None):
    """Generate the final report with streaming - yields content chunks as they arrive.
    
    Yields:
        str: Content chunks as they are generated by the LLM
    """
    # Import the streaming function
    try:
        from .providers import chat_complete_stream
    except ImportError:
        try:
            from deep_wide_research.providers import chat_complete_stream
        except ImportError:
            from providers import chat_complete_stream
    
    findings = "\n".join(state.get("notes") or [])
    system_message = {"role": "system", "content": final_report_generation_prompt.format(date=_today_str())}
    # User message: includes the original user question and raw_notes JSON (injected into messages by engine)
    user_payload = {
        "role": "user",
        "content": (
            "<Messages>\n" +
            "\n\n".join(f"{m['role'].upper()}: {m['content']}" for m in state.get("messages", [])) +
            "\n</Messages>\n\n" +
            "<Findings>\n" + findings + "\n</Findings>"
        )
    }

    # Optional: print debug logs
    print(system_message["content"])
    print(user_payload["content"])

    async for chunk in chat_complete_stream(
        cfg.final_report_model,
        [system_message, user_payload],
        cfg.final_report_model_max_tokens,
        api_keys,
    ):
        yield chunk


